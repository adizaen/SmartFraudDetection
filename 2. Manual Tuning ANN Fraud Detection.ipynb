{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data dataset asli untuk kemudian dilakukan preprocessing data\n",
    "\n",
    "https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi untuk validasi nama kolom\n",
    "\n",
    "def HeaderFile():\n",
    "    list = []\n",
    "    \n",
    "    # perulangan untuk menghapus karakter double quote(\"\")\n",
    "    for col in df.columns:\n",
    "        col = col.replace('\"', '')\n",
    "        list.append(col)\n",
    "    \n",
    "    # mengganti kolom pada dataframe dengan kolom yang telah divalidasi\n",
    "    df.columns = list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import data csv\n",
    "\n",
    "df = pd.read_csv('fraud_data.csv', quoting = csv.QUOTE_NONE)\n",
    "HeaderFile()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proses untuk membersihkan data dan mentransformasikan data sebelum dilakukan klasifikasi. Langkah-langkah yang dilakukan pada tahap ini yaitu mengisi kolom kosong, menghapus kolom dengan nilai konstan, dan normalisasi ke dalam range 0 dan 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi untuk menangani kolom kosong\n",
    "\n",
    "def HandlingMissingValue():\n",
    "    \n",
    "    # cek apakah ada nilai kosong atau tidak\n",
    "    isMissing = df.isnull().values.any()\n",
    "    \n",
    "    # jika ada nilai kosong (True), maka dilakukan proses fillna menggunakan median\n",
    "    if (isMissing == True):\n",
    "        \n",
    "        # membuat list kolom yang memiliki nilai kosong\n",
    "        missColumn = df.columns[df.isnull().any()].tolist() \n",
    "        \n",
    "        # perulangan untuk proses fillna\n",
    "        for col in missColumn:\n",
    "            df.fillna({\n",
    "                col: df[col].median()\n",
    "            }, inplace = True)\n",
    "        \n",
    "        print(\"Successfully handle missing values!\")\n",
    "        print(missColumn)\n",
    "    else:\n",
    "        print(\"Your data is complete! No handling missing value required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi untuk menangani kolom dengan nilai konstan\n",
    "\n",
    "def HandlingConstantValue():\n",
    "    constantValue = []\n",
    "    \n",
    "    # mengecek jumlah data berbeda (unik) suatu kolom. Jika bernilai 1, maka kolom tersebut bernilai konstan (tetap)\n",
    "    for col in df.columns:\n",
    "        if (df[col].nunique() == 1):\n",
    "            constantValue.append(col)\n",
    "        \n",
    "    # jika ada kolom dengan nilai konstan, maka akan dihapus\n",
    "    if (len(constantValue) > 0):\n",
    "        for col in constantValue:\n",
    "            # menghapus kolom yang bernilai konstan\n",
    "            df.drop(col, inplace = True, axis = 1)\n",
    "        \n",
    "        print(\"Successfully handle constant values!\")\n",
    "        print(constantValue)\n",
    "    else:\n",
    "        print(\"Your data is good! No handling constant value required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi untuk normalisasi menggunakan Z Score\n",
    "\n",
    "def Normalization(dataFrame):\n",
    "    # membuat object dari class StandardScaler()\n",
    "    std_scaler = StandardScaler()\n",
    "    \n",
    "    # menyimpan list kolom selain kolom kelas\n",
    "    target = \"flag_transaksi_fraud\"\n",
    "    classColumn = dataFrame[target]\n",
    "    \n",
    "    cols = dataFrame.columns.tolist()\n",
    "    cols = [c for c in cols if c not in [target]]\n",
    "    \n",
    "    # membagi dataframe fitur dan dataframe kelas\n",
    "    targetColumn = dataFrame[cols]\n",
    "    \n",
    "    # proses fit (mencari nilai rerata dan standar deviasi) dan transform (menerapkan ke data)\n",
    "    df_std = pd.DataFrame(std_scaler.fit_transform(targetColumn), columns = cols)\n",
    "    \n",
    "    # menggabungkan dataframe hasil normalisasi dengan kolom target\n",
    "    frameClass = pd.DataFrame(classColumn)\n",
    "    df_std[target] = frameClass\n",
    "    \n",
    "    print(\"Data normalization complete!\")\n",
    "    \n",
    "    # mengembalikan nilai hasil normalisasi\n",
    "    return df_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HandlingMissingValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HandlingConstantValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Normalization(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAMPLING DATA DENGAN SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "cols = df.columns.tolist()\n",
    "cols = [c for c in cols if c not in [\"flag_transaksi_fraud\"]]\n",
    "target = \"flag_transaksi_fraud\"\n",
    "\n",
    "#define X and Y\n",
    "X = df[cols]\n",
    "Y = df[target]\n",
    "\n",
    "#smote\n",
    "X_smote, Y_smote = sm.fit_resample(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "test = pd.DataFrame(Y_smote, columns = ['flag_transaksi_fraud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#visualizing smote results\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(13,4.5))\n",
    "sns.countplot(x=\"flag_transaksi_fraud\", data=df, ax=axs[0])\n",
    "sns.countplot(x=\"flag_transaksi_fraud\", data=test, ax=axs[1])\n",
    "\n",
    "fig.suptitle(\"Class repartition before and after smote\")\n",
    "a1=fig.axes[0]\n",
    "a1.set_title(\"Before\")\n",
    "a2=fig.axes[1]\n",
    "a2.set_title(\"After\")\n",
    "\n",
    "print('Before SMOTE')\n",
    "print('--------------------')\n",
    "print('Normal Transactions \\t: ', (Y.values == 0).sum())\n",
    "print('Fraud Transactions \\t: ', (Y.values == 1).sum())\n",
    "\n",
    "print('\\nAfter SMOTE')\n",
    "print('--------------------')\n",
    "print('Normal Transactions \\t: ', (test.values == 0).sum())\n",
    "print('Fraud Transactions \\t: ', (test.values == 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING NN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, Y_smote, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANUAL TUNING HIDDEN LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "\n",
    "def TuningHiddenLayer():\n",
    "    # import library for count training time\n",
    "    import time\n",
    "    \n",
    "    List_Time = []\n",
    "    List_Accuracy = []\n",
    "    List_Precission = []\n",
    "    List_Recall = []\n",
    "    List_Specificity = []\n",
    "    List_F1_Score = []\n",
    "    List_Error = []\n",
    "    List_AUC = []\n",
    "\n",
    "    for x in range(5):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(512, activation='relu', input_dim=24))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.01) #optimizer\n",
    "        model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy']) #metrics\n",
    "\n",
    "        earlystopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "        history = model.fit(X_train.values, y_train.values, epochs = 100, batch_size=32, validation_split = 0.20, verbose = 0, callbacks = [earlystopper])\n",
    "        history_dict = history.history\n",
    "        \n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "\n",
    "        #predictions\n",
    "        y_pred_nn = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "        #CM matrix\n",
    "        matrix_nn = confusion_matrix(y_test, y_pred_nn)\n",
    "        tp, fn, fp, tn = matrix_nn.reshape(-1)\n",
    "\n",
    "        accuracy = ((tn+tp)/(tn+tp+fp+fn)) * 100\n",
    "        precission = (tp/(tp+fp)) * 100\n",
    "        recall = (tp/(tp+fn)) * 100\n",
    "        specificity = (tn/(tn+fp)) * 100\n",
    "        f1_score = 2 * (recall * precission) / (recall + precission)\n",
    "        error = 100 - accuracy\n",
    "\n",
    "        #AUC\n",
    "        y_pred_nn_proba = model.predict(X_test)\n",
    "        fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test,y_pred_nn_proba)\n",
    "        auc_keras = auc(fpr_keras, tpr_keras) * 100\n",
    "\n",
    "        List_Time.append(training_time)\n",
    "        List_Accuracy.append(accuracy)\n",
    "        List_Precission.append(precission)\n",
    "        List_Recall.append(recall)\n",
    "        List_Specificity.append(specificity)\n",
    "        List_F1_Score.append(f1_score)\n",
    "        List_Error.append(error)\n",
    "        List_AUC.append(auc_keras)\n",
    "        \n",
    "    print(List_Time)\n",
    "    print(List_Accuracy)\n",
    "    print(List_Precission)\n",
    "    print(List_Recall)\n",
    "    print(List_Specificity)\n",
    "    print(List_F1_Score)\n",
    "    print(List_Error)\n",
    "    print(List_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TuningHiddenLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANUAL TUNING BATCH SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "\n",
    "def TuningBatchSize(numBatchSize):\n",
    "     # import library for count training time\n",
    "    import time\n",
    "    \n",
    "    List_Time = []\n",
    "    List_Accuracy = []\n",
    "    List_Precission = []\n",
    "    List_Recall = []\n",
    "    List_Specificity = []\n",
    "    List_F1_Score = []\n",
    "    List_Error = []\n",
    "    List_AUC = []\n",
    "\n",
    "    for x in range(5):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(512, activation='relu', input_dim=24))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.01) #optimizer\n",
    "        model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy']) #metrics\n",
    "\n",
    "        earlystopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "        history = model.fit(X_train.values, y_train.values, epochs = 100, batch_size = numBatchSize, validation_split = 0.20, verbose = 0, callbacks = [earlystopper])\n",
    "        history_dict = history.history\n",
    "        \n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "\n",
    "        #predictions\n",
    "        y_pred_nn = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "        #CM matrix\n",
    "        matrix_nn = confusion_matrix(y_test, y_pred_nn)\n",
    "        tp, fn, fp, tn = matrix_nn.reshape(-1)\n",
    "\n",
    "        accuracy = ((tn+tp)/(tn+tp+fp+fn)) * 100\n",
    "        precission = (tp/(tp+fp)) * 100\n",
    "        recall = (tp/(tp+fn)) * 100\n",
    "        specificity = (tn/(tn+fp)) * 100\n",
    "        f1_score = 2 * (recall * precission) / (recall + precission)\n",
    "        error = 100 - accuracy\n",
    "\n",
    "        #AUC\n",
    "        y_pred_nn_proba = model.predict(X_test)\n",
    "        fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test,y_pred_nn_proba)\n",
    "        auc_keras = auc(fpr_keras, tpr_keras) * 100\n",
    "\n",
    "        List_Time.append(training_time)\n",
    "        List_Accuracy.append(accuracy)\n",
    "        List_Precission.append(precission)\n",
    "        List_Recall.append(recall)\n",
    "        List_Specificity.append(specificity)\n",
    "        List_F1_Score.append(f1_score)\n",
    "        List_Error.append(error)\n",
    "        List_AUC.append(auc_keras)\n",
    "        \n",
    "    print(List_Time)\n",
    "    print(List_Accuracy)\n",
    "    print(List_Precission)\n",
    "    print(List_Recall)\n",
    "    print(List_Specificity)\n",
    "    print(List_F1_Score)\n",
    "    print(List_Error)\n",
    "    print(List_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TuningBatchSize(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TuningBatchSize(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TuningBatchSize(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TuningBatchSize(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TuningBatchSize(256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANUAL TUNING LEARNING RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "\n",
    "def TuningLearningRate(numLearningRate):\n",
    "    # import library for count training time\n",
    "    import time\n",
    "        \n",
    "    List_Time = []\n",
    "    List_Accuracy = []\n",
    "    List_Precission = []\n",
    "    List_Recall = []\n",
    "    List_Specificity = []\n",
    "    List_F1_Score = []\n",
    "    List_Error = []\n",
    "    List_AUC = []\n",
    "\n",
    "    for x in range(5):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(512, activation='relu', input_dim=24))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate = numLearningRate) #optimizer\n",
    "        model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy']) #metrics\n",
    "\n",
    "        earlystopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "        history = model.fit(X_train.values, y_train.values, epochs = 100, batch_size = 128, validation_split = 0.20, verbose = 0, callbacks = [earlystopper])\n",
    "        history_dict = history.history\n",
    "        \n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "\n",
    "        #predictions\n",
    "        y_pred_nn = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "        #CM matrix\n",
    "        matrix_nn = confusion_matrix(y_test, y_pred_nn)\n",
    "        tp, fn, fp, tn = matrix_nn.reshape(-1)\n",
    "\n",
    "        accuracy = ((tn+tp)/(tn+tp+fp+fn)) * 100\n",
    "        precission = (tp/(tp+fp)) * 100\n",
    "        recall = (tp/(tp+fn)) * 100\n",
    "        specificity = (tn/(tn+fp)) * 100\n",
    "        f1_score = 2 * (recall * precission) / (recall + precission)\n",
    "        error = 100 - accuracy\n",
    "\n",
    "        #AUC\n",
    "        y_pred_nn_proba = model.predict(X_test)\n",
    "        fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test,y_pred_nn_proba)\n",
    "        auc_keras = auc(fpr_keras, tpr_keras) * 100\n",
    "\n",
    "        List_Time.append(training_time)\n",
    "        List_Accuracy.append(accuracy)\n",
    "        List_Precission.append(precission)\n",
    "        List_Recall.append(recall)\n",
    "        List_Specificity.append(specificity)\n",
    "        List_F1_Score.append(f1_score)\n",
    "        List_Error.append(error)\n",
    "        List_AUC.append(auc_keras)\n",
    "        \n",
    "    print(List_Time)\n",
    "    print(List_Accuracy)\n",
    "    print(List_Precission)\n",
    "    print(List_Recall)\n",
    "    print(List_Specificity)\n",
    "    print(List_F1_Score)\n",
    "    print(List_Error)\n",
    "    print(List_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TuningLearningRate(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TuningLearningRate(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TuningLearningRate(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TuningLearningRate(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TuningLearningRate(0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # FINAL TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "\n",
    "def BuildModel():\n",
    "    # import library for count training time\n",
    "    import time\n",
    "    \n",
    "    # BEST PARAMETER\n",
    "    learningRate = 0.0001\n",
    "    batchSize = 128\n",
    "    \n",
    "    List_Time = []\n",
    "    List_Accuracy = []\n",
    "    List_Precission = []\n",
    "    List_Recall = []\n",
    "    List_Specificity = []\n",
    "    List_F1_Score = []\n",
    "    List_Error = []\n",
    "    List_AUC = []\n",
    "\n",
    "    for x in range(10):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(512, activation='relu', input_dim=24))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate = learningRate) #optimizer\n",
    "        model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy']) #metrics\n",
    "\n",
    "        earlystopper = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 15, verbose = 1, mode = 'auto', baseline = None, restore_best_weights=False)\n",
    "\n",
    "        history = model.fit(X_train.values, y_train.values, epochs = 100, batch_size = batchSize, validation_split = 0.20, verbose = 0, callbacks = [earlystopper])\n",
    "        history_dict = history.history\n",
    "        \n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "\n",
    "        #predictions\n",
    "        y_pred_nn = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "        #CM matrix\n",
    "        matrix_nn = confusion_matrix(y_test, y_pred_nn)\n",
    "        tp, fn, fp, tn = matrix_nn.reshape(-1)\n",
    "\n",
    "        accuracy = ((tn+tp)/(tn+tp+fp+fn)) * 100\n",
    "        precission = (tp/(tp+fp)) * 100\n",
    "        recall = (tp/(tp+fn)) * 100\n",
    "        specificity = (tn/(tn+fp)) * 100\n",
    "        f1_score = 2 * (recall * precission) / (recall + precission)\n",
    "        error = 100 - accuracy\n",
    "\n",
    "        #AUC\n",
    "        y_pred_nn_proba = model.predict(X_test)\n",
    "        fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test,y_pred_nn_proba)\n",
    "        auc_keras = auc(fpr_keras, tpr_keras) * 100\n",
    "\n",
    "        List_Time.append(training_time)\n",
    "        List_Accuracy.append(accuracy)\n",
    "        List_Precission.append(precission)\n",
    "        List_Recall.append(recall)\n",
    "        List_Specificity.append(specificity)\n",
    "        List_F1_Score.append(f1_score)\n",
    "        List_Error.append(error)\n",
    "        List_AUC.append(auc_keras)\n",
    "        \n",
    "    print(List_Time)\n",
    "    print(List_Accuracy)\n",
    "    print(List_Precission)\n",
    "    print(List_Recall)\n",
    "    print(List_Specificity)\n",
    "    print(List_F1_Score)\n",
    "    print(List_Error)\n",
    "    print(List_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BuildModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
